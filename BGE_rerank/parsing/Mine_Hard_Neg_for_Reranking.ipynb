{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65731371-eb76-4317-8b00-d5b16fdeddaa",
   "metadata": {},
   "source": [
    "Top 10 to 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124702a9-f6f8-4136-b309-063e255d98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import h5py\n",
    "import faiss\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# Load the JSON files with article summaries and references\n",
    "input_path_queries = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/LONG_CTG_id_text_refs_train.json'\n",
    "input_path_articles = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Retrievial/v2_/NEW_PM_id_text.json'\n",
    "embeddings_path = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/CLS_NEW_PM_id_text_W_BGE_L.h5'\n",
    "output_path = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/top10_17_retrieved_articles.json'\n",
    "\n",
    "# Set the variables\n",
    "NUM_QUERIES = 21680\n",
    "TOP_K = 27  # Retrieve at least the top 17 results to access indices 10 to 17\n",
    "\n",
    "# Load queries\n",
    "with open(input_path_queries, 'r') as f:\n",
    "    queries = json.load(f)\n",
    "# Limit the number of queries if desired\n",
    "queries = queries[:NUM_QUERIES]\n",
    "\n",
    "# Load article metadata (to map index -> article_id)\n",
    "with open(input_path_articles, 'r') as f:\n",
    "    articles = json.load(f)\n",
    "article_ids = [article['article_id'] for article in articles]\n",
    "\n",
    "# ---------------------------\n",
    "# Load Model in Half Precision\n",
    "# ---------------------------\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.half()       # Convert model weights to fp16\n",
    "model.to('cuda')   # Move model to GPU\n",
    "model.eval()       # Set model to evaluation mode\n",
    "\n",
    "# ---------------------------\n",
    "# Function: get query CLS embedding\n",
    "# ---------------------------\n",
    "def get_query_embedding(text, tokenizer, model):\n",
    "    # Tokenize and move to GPU\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "        # CLS pooling: use the [CLS] token (first token) representation\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    # Convert to NumPy (float16 for consistency)\n",
    "    return cls_embedding.squeeze().cpu().numpy().astype('float16')\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load article embeddings from HDF5\n",
    "# 2. Build FAISS index\n",
    "# ---------------------------\n",
    "print(\"Loading article embeddings from HDF5...\")\n",
    "embeddings = []\n",
    "article_id_list = []\n",
    "\n",
    "with h5py.File(embeddings_path, 'r') as hf:\n",
    "    for article_id in hf.keys():\n",
    "        emb = hf[article_id][:]\n",
    "        embeddings.append(emb)\n",
    "        article_id_list.append(article_id)\n",
    "\n",
    "# Convert to NumPy array\n",
    "embeddings = np.array(embeddings)  # shape: (num_articles, hidden_size)\n",
    "# By default this might be float16 if you didn't cast. Ensure it's float32 or float16 as needed:\n",
    "embeddings = embeddings.astype('float32')  # or keep float16 if you prefer\n",
    "\n",
    "# IMPORTANT: We must maintain consistent ordering between embeddings and article_id_list.\n",
    "# So after sorting them we can do a quick re-check. But if your h5 creation was random order,\n",
    "# you either keep them as is or you match them carefully with \"article_ids\" from `PM_id_text.json`.\n",
    "\n",
    "# For a quick approach, we'll just treat \"article_id_list\" as the official ordering.\n",
    "# If you want to map to your \"articles\" from JSON, do so carefully. One approach is:\n",
    "#  - Sort article_id_list if needed,\n",
    "#  - Build a lookup table (dictionary) from article_id_list to index.\n",
    "\n",
    "print(\"Normalizing article embeddings for cosine similarity...\")\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / (norms + 1e-9)  # to avoid divide-by-zero\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # inner product index\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index created. Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# We'll create a dictionary to quickly map from index -> article_id\n",
    "idx_to_article_id = dict(enumerate(article_id_list))\n",
    "\n",
    "# ---------------------------\n",
    "# Retrieval Loop\n",
    "# ---------------------------\n",
    "retrieval_results = []\n",
    "\n",
    "print(\"Processing queries (CLS pooling for each query, FAISS retrieval)...\")\n",
    "for query_data in tqdm(queries, desc=\"Processing Queries\"):\n",
    "    query_text = query_data['summary']\n",
    "    query_ref = query_data['ref']\n",
    "    ntc_id = query_data.get('ntcId', 'N/A')\n",
    "\n",
    "    # Get the query embedding\n",
    "    q_emb = get_query_embedding(query_text, tokenizer, model)\n",
    "    # Normalize for cosine similarity\n",
    "    q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-9)\n",
    "\n",
    "    # FAISS expects shape (1, dim), cast to float32 for IndexFlatIP\n",
    "    q_emb_2d = np.expand_dims(q_emb, axis=0).astype('float32')\n",
    "\n",
    "    # Search top K\n",
    "    D, I = index.search(q_emb_2d, TOP_K)\n",
    "    # I has shape (1, TOP_K)\n",
    "    top_indices = I[0]\n",
    "\n",
    "    # Get article IDs for the 10th through 17th results\n",
    "    # (Indices [9:17] = 10th through 17th in 0-based indexing)\n",
    "    top_article_ids = [idx_to_article_id[i] for i in top_indices[10:17]]\n",
    "\n",
    "    retrieval_results.append({\n",
    "        'ntcId': ntc_id,\n",
    "        'ref': query_ref,\n",
    "        'top_19_25_retrieved_ids': top_article_ids\n",
    "    })\n",
    "\n",
    "# ---------------------------\n",
    "# Save results\n",
    "# ---------------------------\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(retrieval_results, f, indent=4)\n",
    "\n",
    "print(f\"Done! Retrieval results saved to {output_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8c50122-46f0-45fe-bafc-8fbe14bb1383",
   "metadata": {},
   "source": [
    "Remove the same ref "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de8de3-6df5-4a0e-bc5a-7dc4003d653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the JSON file with retrieval results\n",
    "input_path = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/top10_17_retrieved_articles.json'\n",
    "\n",
    "with open(input_path, 'r') as f:\n",
    "    retrieval_results = json.load(f)\n",
    "\n",
    "# Iterate over each result and modify the top 5 retrieved IDs list\n",
    "for result in retrieval_results:\n",
    "    ref = str(result['ref'])\n",
    "    top_5_ids = result['top_10_17_retrieved_ids']\n",
    "\n",
    "    # If the reference is in the top 5 retrieved IDs, remove it\n",
    "    if ref in top_5_ids:\n",
    "        top_5_ids.remove(ref)\n",
    "    else:\n",
    "        # If the reference is not in the top 5, remove a random element\n",
    "        if len(top_5_ids) > 0:\n",
    "            top_5_ids.pop(random.randrange(len(top_5_ids)))\n",
    "\n",
    "    # Update the result with the modified list\n",
    "    result['top_10_17_retrieved_ids'] = top_5_ids\n",
    "\n",
    "# Save the modified retrieval results to the same JSON file\n",
    "with open(input_path, 'w') as f:\n",
    "    json.dump(retrieval_results, f, indent=5)\n",
    "\n",
    "print(f\"Modified results saved to {input_path}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c3e1c50-24c5-4119-af07-c101cb250902",
   "metadata": {},
   "source": [
    "Create training file with HN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1795d6-daa9-41f6-b11c-e42288ac5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define file paths\n",
    "long_ctg_file = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/LONG_CTG_id_text_refs_train.json'\n",
    "top5_articles_file = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/top10_17_retrieved_articles.json'\n",
    "pm_id_text_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Retrievial/v2_/NEW_PM_id_text.json'\n",
    "output_file = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/rerank_training_HN_10_17.jsonl'\n",
    "\n",
    "# Load data from files\n",
    "with open(long_ctg_file, 'r') as f:\n",
    "    long_ctg_data = json.load(f)\n",
    "\n",
    "with open(top5_articles_file, 'r') as f:\n",
    "    top5_articles_data = json.load(f)\n",
    "\n",
    "with open(pm_id_text_file, 'r') as f:\n",
    "    pm_id_text_data = json.load(f)\n",
    "\n",
    "# Create dictionaries for quick lookups\n",
    "ref_to_summary = {entry['ref']: entry['summary'] for entry in long_ctg_data}\n",
    "article_id_to_text = {entry['article_id']: entry['text'] for entry in pm_id_text_data}\n",
    "\n",
    "# Process each entry in top5_retrieved_articles.json and create the output\n",
    "output_data = []\n",
    "for entry in top5_articles_data:\n",
    "    ref = entry['ref']\n",
    "    top_5_ids = entry['top_10_17_retrieved_ids']\n",
    "\n",
    "    # Get query (summary)\n",
    "    query = ref_to_summary.get(ref, None)\n",
    "    if query is None:\n",
    "        continue\n",
    "\n",
    "    # Get positive text\n",
    "    pos = article_id_to_text.get(str(ref), None)\n",
    "    if pos is None:\n",
    "        continue\n",
    "\n",
    "    # Get negative texts\n",
    "    neg = [article_id_to_text.get(str(article_id), None) for article_id in top_5_ids]\n",
    "    neg = [text for text in neg if text is not None]\n",
    "\n",
    "    # Append to output data\n",
    "    output_data.append({\"query\": query, \"pos\": [pos], \"neg\": neg})\n",
    "\n",
    "# Write output to a JSONL file\n",
    "with open(output_file, 'w') as f:\n",
    "    for entry in output_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Output written to {output_file}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2375a60-3c57-492e-8bc7-83fe4ed51426",
   "metadata": {},
   "source": [
    "Create training without HN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef717c82-e8bd-4381-a170-8d8c999de4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define file paths\n",
    "long_ctg_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Retrievial/Complete_PP/LONG_CTG_id_text_refs_train.json'\n",
    "top5_articles_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Retrievial/Complete_PP/top5_retrieved_articles.json'\n",
    "pm_id_text_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Retrievial/v2_/PM_id_text.json'\n",
    "output_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Retrievial/Complete_PP/rerank_training_no_HN.jsonl'\n",
    "\n",
    "# Load data from files\n",
    "with open(long_ctg_file, 'r') as f:\n",
    "    long_ctg_data = json.load(f)\n",
    "\n",
    "with open(top5_articles_file, 'r') as f:\n",
    "    top5_articles_data = json.load(f)\n",
    "\n",
    "with open(pm_id_text_file, 'r') as f:\n",
    "    pm_id_text_data = json.load(f)\n",
    "\n",
    "# Create dictionaries for quick lookups\n",
    "ref_to_summary = {entry['ref']: entry['summary'] for entry in long_ctg_data}\n",
    "article_id_to_text = {entry['article_id']: entry['text'] for entry in pm_id_text_data}\n",
    "\n",
    "# Process each entry in top5_retrieved_articles.json and create the output\n",
    "output_data = []\n",
    "for entry in top5_articles_data:\n",
    "    ref = entry['ref']\n",
    "    top_5_ids = entry['top_5_retrieved_ids']\n",
    "\n",
    "    # Get query (summary)\n",
    "    query = ref_to_summary.get(ref, None)\n",
    "    if query is None:\n",
    "        continue\n",
    "\n",
    "    # Get positive text\n",
    "    pos = article_id_to_text.get(str(ref), None)\n",
    "    if pos is None:\n",
    "        continue\n",
    "\n",
    "    # Get negative texts\n",
    "    neg = [\"\"]\n",
    "\n",
    "    # Append to output data\n",
    "    output_data.append({\"query\": query, \"pos\": [pos], \"neg\": neg})\n",
    "\n",
    "# Write output to a JSONL file\n",
    "with open(output_file, 'w') as f:\n",
    "    for entry in output_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Output written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19e4e5-6cbf-42cf-b212-58e917db235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "output_path = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/unique_ct_terms_weight.json'\n",
    "\n",
    "with open(output_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Print the length of the file\n",
    "print(f\"Number of entries in the file: {len(data)}\")\n",
    "\n",
    "# Print the first 2 entries (head)\n",
    "print(data[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
